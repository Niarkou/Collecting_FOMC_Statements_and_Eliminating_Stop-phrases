{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time, os, re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "selenium_filepath = \"C:\\GIT\\SELENIUM_DRIVERS\\chromedriver_win32\\chromedriver.exe\"\n",
    "\n",
    "start_yyyymmdd = \"01/01/1994\"\n",
    "end_yyyymmdd = \"09/24/2022\"\n",
    "save_root_dir = './Minutes'\n",
    "\n",
    "url = \"https://www.federalreserve.gov/monetarypolicy/materials/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(selenium_filepath)\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set start date\n",
    "start_date = driver.find_element_by_name(\"startmodel\")\n",
    "start_date.clear()\n",
    "start_date.send_keys(start_yyyymmdd)\n",
    "\n",
    "# set end date\n",
    "end_date = driver.find_element_by_name(\"endmodel\")\n",
    "end_date.clear()\n",
    "end_date.send_keys(end_yyyymmdd)\n",
    "\n",
    "# select policy statements\n",
    "statement_checkbox = driver.find_element_by_xpath(\"//label/input[contains(..,'Minutes (1993-Present)')]\")\n",
    "statement_checkbox.click()\n",
    "\n",
    "# apply filter\n",
    "submit = driver.find_element_by_css_selector(\".btn.btn-primary\")\n",
    "submit.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the page control row\n",
    "pagination = driver.find_element_by_class_name('pagination')\n",
    "\n",
    "# go to the last page to find the largest page number\n",
    "last_page = pagination.find_element_by_link_text('Last')\n",
    "last_page.click()\n",
    "pages = pagination.text.split('\\n')\n",
    "largest_page = int(pages[-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of URLs: 250\n"
     ]
    }
   ],
   "source": [
    "statement_url_list = []\n",
    "# go back to first page and start the loop\n",
    "first_page = pagination.find_element_by_link_text('First')\n",
    "first_page.click()\n",
    "next_page = pagination.find_element_by_link_text('Next')\n",
    "for i in range(largest_page):\n",
    "    # now to get the items inside\n",
    "    main = driver.find_element_by_css_selector(\".panel.panel-default\") # get the app panel\n",
    "    material_types = main.find_elements_by_css_selector(\".fomc-meeting__month.col-xs-5.col-sm-3.col-md-4\") # get the 2nd col\n",
    "    material_types = [element.text for element in material_types] # to get the words\n",
    "    material_links = main.find_elements_by_css_selector(\".fomc-meeting__month.col-xs-5.col-sm-3.col-md-2\") # get the 3rd col\n",
    "    \n",
    "    html_elements = []\n",
    "    for element in material_links:\n",
    "        try:\n",
    "            html_elements.append(element.find_element_by_link_text('HTML'))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # add url to statement_url_list if it is a statement\n",
    "    statement_url_list.extend([html_elements[i].get_attribute('href') for i, j in enumerate(material_types) if j=='Minutes'])\n",
    "    next_page.click()\n",
    "print('Number of URLs: {}'.format(len(statement_url_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrpae texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_for_a_statement_from_201201_to_202209(soup):\n",
    "    return soup.find('div', class_ = 'col-xs-12 col-sm-8 col-md-9').text.strip()\n",
    "\n",
    "def get_text_for_a_statement_from_200710_to_201112(soup):\n",
    "    return soup.find('div', id=\"leftText\").text.strip()\n",
    "\n",
    "def get_text_for_a_statement_from_199601_to_200709(soup):\n",
    "    return '\\n'.join([item.text.strip() for item in soup.select('table td')])\n",
    "\n",
    "def get_text_for_a_statement_from_199401_to_199512(soup):\n",
    "    return soup.find('div', id=\"content\").text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [02:16<00:00,  1.83it/s]\n"
     ]
    }
   ],
   "source": [
    "for statement_url in tqdm(statement_url_list):\n",
    "    statement_resp = requests.get(statement_url)\n",
    "    statement_soup = BeautifulSoup(statement_resp.content, 'lxml')\n",
    "    \n",
    "    for item in re.findall('[0-9]+', statement_url):\n",
    "        if len(item)==8:\n",
    "            yyyymmdd = item\n",
    "    \n",
    "    yearmonth = int(yyyymmdd[:6])\n",
    "    if yearmonth >= 201201:\n",
    "        article = get_text_for_a_statement_from_201201_to_202209(statement_soup)\n",
    "    elif yearmonth >= 200710:\n",
    "        article = get_text_for_a_statement_from_200710_to_201112(statement_soup)\n",
    "    elif yearmonth >= 199601:\n",
    "        article = get_text_for_a_statement_from_199601_to_200709(statement_soup)    \n",
    "    else:\n",
    "        article = get_text_for_a_statement_from_199401_to_199512(statement_soup)\n",
    "        \n",
    "    save_dir = os.path.join(save_root_dir, yyyymmdd[:4])\n",
    "    if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "    save_filepath = os.path.join(save_dir, '{}.txt'.format(yyyymmdd))\n",
    "    with open(save_filepath, \"w\", encoding='utf-8-sig') as file:\n",
    "        file.write(\"{}\\n\\n\".format(statement_url))\n",
    "        file.write(article)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
