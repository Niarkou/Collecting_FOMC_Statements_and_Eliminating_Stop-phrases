{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, re, pickle, argparse, shutil\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "import requests\n",
    "from daterangeparser import parse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "start_mmddyyyy = \"01/01/1994\"\n",
    "end_mmddyyyy = \"08/22/2023\"\n",
    "\n",
    "chromedriver_filepath = \"\"\n",
    "save_root_dir = './Statements'\n",
    "\n",
    "url = \"https://www.federalreserve.gov/monetarypolicy/materials/\"\n",
    "\n",
    "\n",
    "def prepare_resources_for_scraping(chromedriver_filepath, url, start_mmddyyyy, end_mmddyyyy):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.headless = False\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # set start date\n",
    "    start_date = driver.find_element(By.NAME, \"startmodel\")\n",
    "    start_date.clear()\n",
    "    start_date.send_keys(start_mmddyyyy)\n",
    "\n",
    "    # set end date\n",
    "    end_date = driver.find_element(By.NAME, \"endmodel\")\n",
    "    end_date.clear()\n",
    "    end_date.send_keys(end_mmddyyyy)\n",
    "\n",
    "    # select policy statements\n",
    "    statement_checkbox = driver.find_element(By.XPATH, \"//label/input[contains(..,'Policy Statements')]\")\n",
    "    statement_checkbox.click()\n",
    "\n",
    "    # apply filter\n",
    "    submit = driver.find_element(By.CSS_SELECTOR, \".btn.btn-primary\")\n",
    "    submit.click()\n",
    "    \n",
    "    # get the page control row\n",
    "    pagination = driver.find_element(By.CLASS_NAME, 'pagination')\n",
    "\n",
    "    # go to the last page to find the largest page number\n",
    "    last_page = pagination.find_element(By.LINK_TEXT, 'Last')\n",
    "    last_page.click()\n",
    "    pages = pagination.text.split('\\n')\n",
    "    largest_page = int(pages[-3])\n",
    "    \n",
    "    return driver, pagination, largest_page\n",
    "\n",
    "def extract_meetingdate_documentdate_statementurl(soup):\n",
    "    meeting_date = soup.select('strong')[0].text\n",
    "    document_date = soup.select('em')[0].text\n",
    "    statement_url = 'https://www.federalreserve.gov/{}'.format([item for item in soup.select('a') if 'HTML' in item.text][0]['href'])\n",
    "    return meeting_date, document_date, statement_url\n",
    "\n",
    "def scrape_URLs_and_meeting_dates_and_document_dates(driver, pagination, largest_page):\n",
    "    meeting_date_list, document_date_list, statement_url_list = [], [], []\n",
    "    # go back to first page and start the loop\n",
    "    first_page = pagination.find_element(By.LINK_TEXT, 'First')\n",
    "    first_page.click()\n",
    "    next_page = pagination.find_element(By.LINK_TEXT, 'Next')\n",
    "    \n",
    "    for _ in range(largest_page):\n",
    "        driver.find_element(By.CSS_SELECTOR, \".panel.panel-default\") \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        rows = soup.select('div.row.fomc-meeting')[1:]\n",
    "        for one_row in rows:\n",
    "            if one_row.select('.fomc-meeting__month.col-xs-5.col-sm-3.col-md-4')[0].text.strip()=='Statement':\n",
    "                # Extract statements written in HTML format\n",
    "                meeting_date, document_date, statement_url = extract_meetingdate_documentdate_statementurl(one_row)\n",
    "                meeting_date_list.append(meeting_date)\n",
    "                document_date_list.append(document_date)\n",
    "                statement_url_list.append(statement_url)\n",
    "        \n",
    "        next_page.click()\n",
    "    print('Number of URLs: {}'.format(len(statement_url_list)))\n",
    "    \n",
    "    return statement_url_list, meeting_date_list, document_date_list\n",
    "\n",
    "def get_text_for_a_statement_from_2006_to_2022(soup):\n",
    "    return soup.find('div', class_ = 'col-xs-12 col-sm-8 col-md-8').text.strip()\n",
    "\n",
    "def get_text_for_a_statement_from_1996_to_2005(soup):\n",
    "    return '\\n'.join([item.text.strip() for item in soup.select('table td')])\n",
    "\n",
    "def get_text_for_a_statement_from_1994_to_1995(soup):\n",
    "    return soup.find('div', id=\"content\").text.strip()\n",
    "\n",
    "doublespace_pattern = re.compile(r'\\s+')\n",
    "def remove_doublespaces(document):\n",
    "    return doublespace_pattern.sub(' ', document).strip()\n",
    "\n",
    "stop_phrase_patterns = [re.compile(r'Release Date: [A-z][a-z]{2,8} \\d{1,2}, \\d{4}')\\\n",
    "                       , re.compile('For immediate release')\\\n",
    "                       , re.compile(r'Home \\|.*')\\\n",
    "                       , re.compile(r'\\d{4} Monetary policy')\\\n",
    "                       , re.compile('Implementation Note issued.*')\\\n",
    "                       , re.compile('Frequently Asked Questions.*')\\\n",
    "                       , re.compile('For media inquiries.*')\\\n",
    "                       , re.compile(r'\\(\\d{1,3} KB PDF\\)')]\n",
    "def remove_stop_phrases(document):\n",
    "    for stop_phrase_pattern in stop_phrase_patterns:\n",
    "        document = stop_phrase_pattern.sub(' ', document)\n",
    "        document = remove_doublespaces(document)\n",
    "    return document\n",
    "\n",
    "def extract_begin_end_dates(date_range):\n",
    "    if '-' not in date_range:\n",
    "        parsed, _ = parse(date_range)\n",
    "        return parsed, parsed\n",
    "    \n",
    "    elif '/' in date_range:\n",
    "        begin_month, end_month, begin_date, end_date, year = date_range.replace(',', '').replace('-', ' ').replace('/', ' ').split(' ')\n",
    "        date_range = f'{begin_month} {begin_date}-{end_month} {end_date}, {year}'\n",
    "        return parse(date_range)\n",
    "        \n",
    "    else:\n",
    "        return parse(date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of URLs: 230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "230it [02:11,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 221 unique documents under ./Statements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "driver, pagination, largest_page = prepare_resources_for_scraping(chromedriver_filepath, url, start_mmddyyyy, end_mmddyyyy)\n",
    "statement_url_list, meeting_date_list, document_date_list = scrape_URLs_and_meeting_dates_and_document_dates(driver, pagination, largest_page)\n",
    "\n",
    "doc_count = 0\n",
    "for statement_url, meeting_date, document_date in tqdm(zip(statement_url_list, meeting_date_list, document_date_list)):\n",
    "\n",
    "    # Scrape statements\n",
    "    statement_resp = requests.get(statement_url)\n",
    "    statement_soup = BeautifulSoup(statement_resp.content, 'lxml')\n",
    "\n",
    "    document_date_yyyymmdd = datetime.strftime(datetime.strptime(document_date, \"%B %d, %Y\"), \"%Y%m%d\")\n",
    "    year = int(document_date_yyyymmdd[:4])\n",
    "    if year >= 2006:\n",
    "        doc = get_text_for_a_statement_from_2006_to_2022(statement_soup)\n",
    "    elif year >=1996:\n",
    "        doc = get_text_for_a_statement_from_1996_to_2005(statement_soup)\n",
    "    else:\n",
    "        doc = get_text_for_a_statement_from_1994_to_1995(statement_soup)\n",
    "\n",
    "    # Clean\n",
    "    doc = remove_doublespaces(doc)\n",
    "\n",
    "    # Remove stop-phrases\n",
    "    doc = remove_stop_phrases(doc)\n",
    "\n",
    "    \n",
    "    meeting_date_start, meeting_date_end = extract_begin_end_dates(meeting_date)\n",
    "    meeting_date_start_string = meeting_date_start.strftime(\"%Y-%m-%d\")\n",
    "    meeting_date_end_string = meeting_date_end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Save data\n",
    "    save_dir = os.path.join(save_root_dir, document_date_yyyymmdd[:4])\n",
    "    if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "    save_filepath = os.path.join(save_dir, 'MeetingDate={}-{}_UploadedOn={}.txt'\\\n",
    "                                 .format(meeting_date_start_string, meeting_date_end_string, document_date_yyyymmdd))\n",
    "    with open(save_filepath, \"w\", encoding='utf-8-sig') as file:\n",
    "        file.write(doc)\n",
    "        doc_count += 1\n",
    "\n",
    "print('Saved {} unique documents under {}'.format(len(glob('{}/*/*.txt'.format(save_root_dir))), save_root_dir)) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}